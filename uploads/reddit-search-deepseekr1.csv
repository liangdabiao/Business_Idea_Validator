web-scraper-order,web-scraper-start-url,title,description,link,link-href,reviews,Upvotes,comments
"1749215744-1","https://www.reddit.com/search/?q=deepseek+r1&cId=49713cac-e260-4a3e-9107-9a9824aac864&iId=afc0b3e2-07c1-4e5f-becd-e3bca85e04b8","r/SillyTavernAI","","r/SillyTavernAI","https://www.reddit.com/r/SillyTavernAI/","[{""reviews"":""I thought Gemini flash since 2.0 was tool-using and function calling but when I start a chat my first message gets blocked with an error message saying tool use not supported. I wired in my own custom deep research tool and turned on AccuWeather and RSS, OAI 4o and 4o mini are fine with it, any ideas?""},{""reviews"":""What is the best preset for Gemini 2.5 pro 06 05 right now?""},{""reviews"":""I'm currently using Gemini 2.5 flash preview 04-17.\n    \n      The characters I'm trying to get right are Smaug and Gandalf.\n    \n      For whatever reason, Gemini is good at nearly every character I've tried, but for some reason it's really bad at these two. I've tried several times to get it to speak as they would from the books and movies, but I just keep getting the same two things.\n    \n      Smaug doesn't have dialogue like Smaug, and instead acts like any stereotypical arrogant and \""evil\"" dragon character. He'll ramble on and on with just far too much dialogue about how pathetic he thinks I am, threatening to kill me, how easy it would be to do so, and occasionally going \""Hmph.\"" He also loves to wax poetic about the nature of a dragon being to kill, hoard, and covet gold.\nBut at the same time he doesn't seem like he really wants to kill anyone and is far less deadly than his source version. Kinda pacified. I could sit there and insult him repeatedly and he'd just insult me back about how pathetic I am and that I'm an \""annoying gnat\"".\n    \n      Gandalf talks too much and... just isn't Gandalf. It's best explained with some screenshots:\n    https://preview.redd.it/need-some-help-with-getting-two-lotr-characters-gandalf-and-v0-4qypyn0b775f1.pngThe context of these is I found him I told him I know how Smaug will die and I wish to prevent it because I believe him redeemable. (I know this is foolish it's just a dumb scenario). I know the molten gold trap is only in the movie, and what's odd is that Gandalf here knows of it when he really shouldn't.\n      Not sure why it's so good at others and terrible at these two. Maybe because Gandalf's speech varies from page to page? Sometimes he'll be shouting, other times he'll be making a little joke, other times he'll be completely stoic and of few words, and then other times he's explaining something in great detail, and it's kind of just... combining all of these into one.""},{""reviews"":""One of the big things people are always trying to understand from these megathreads is 'What's the best model I can run on MY hardware?'  As it currently stands it's always a bit of a pain to understand what the best model is for a given VRAM limit.  Can I suggest the following sections?\n    \n        \n      \n      >= 70B\n    \n    \n      \n      32B to 70B\n    \n    \n      \n      16B to 32B\n    \n    \n      \n      8B to 16B\n    \n    \n      \n      < 8B\n    \n    \n      \n      APIs\n    \n    \n      \n      MISC DISCUSSION\n    \n    \n      \n      We could have everyone comment in thread *under* the relevant sections and maybe remove top level comments.\n    \n      I took this salary post as inspiration.  No doubt those threads have some fancy automod scripting going on.  That would be ideal long term but in the short term we could just just do it manually a few times to see how well it works for this sub?  What do you guys think?""},{""reviews"":""I'm using Deepseek R1 0523 with a 163k context size. At what point does the model get sloppy in its writing? As of right now, my prompts are about 20k tokens and it's still running like a charm.""},{""reviews"":""Hi! I’m not a programmer or AI developer, but I’ve been doing something on my own for a while out of passion.\n    \n      I’ve noticed that most AI responses — especially in roleplay or emotional dialogue — tend to sound repetitive, shallow, or generic. They often reuse the same phrases and don’t adapt well to different character personalities like tsundere, kuudere, yandere, etc.\n    \n      So I started collecting and organizing dialogue from games, anime, visual novels, and even NSFW content. I'm manually extracting lines directly from files and scenes, then categorizing them based on tone, personality type, and whether it's SFW or NSFW.\n    \n      I'm trying to build a kind of \""word and emotion library\"" so AI could eventually talk more like real characters, with variety and personality. It’s just something I care about and enjoy working on.\n    \n      My question is: Is this kind of work actually useful for improving AI models? And if yes, where can I send or share this kind of dialogue dataset?\n    \n      I tried giving it to models like Gemini, but it didn’t really help since the model doesn’t seem trained on this kind of expressive or emotional language. I haven’t contacted any open-source teams yet, but maybe I will if I know it’s worth doing.\n    \n      Edit: I should clarify — my main goal isn’t just collecting dialogue, but actually expanding the language and vocabulary AI can use, especially in emotional or roleplay conversations.\n    \n      A lot of current AI responses feel repetitive or shallow, even with good prompts. I want to help models express emotions better and have more variety in how characters talk — not just the same 10 phrases recycled over and over.\n    \n      So this isn’t just about training on what characters say, but how they say it, and giving AI access to a wider, richer way of speaking like real personalities.\n    \n      Any advice would mean a lot — thank you!""},{""reviews"":""The Claude 4 family of models is clearly the most powerful at writing fiction and compelling characters, yet there's no popular benchmark that attests that.\n    \n    \n      \n      If one looks at popular banchmark alone, not only the Claude 4 family of models loses to competiton in coding, logic and memory but it's also overpriced.\n    \n    \n      \n      Despite these shortcomings, we all know where Claude's true trenght resides - creativity - but measuring such strenght is  hard as there are not right or wrong answers in evaluating a model's creativity and ability to reproduce human-like behaviors.\n    \n    \n      \n      Any lesser known benchmarks that align with user experiences with creative writing? If not, how would you design one?""},{""reviews"":""All new model posts must include the following information:\n    \n        \n      \n      Model Name: Cydonia 24B v3\n    \n    \n      \n      Model URL: https://huggingface.co/TheDrummer/Cydonia-24B-v3\n    \n    \n      \n      Model Author: Drummer\n    \n    \n      \n      What's Different/Better: No vision. Uses Mistral 24B 2503.\n    \n    \n      \n      Backend: KoboldCPP\n    \n    \n      \n      Settings: Mistral v7 Tekken (No Meth this time!)\n    \n    \n      \n    \n      \n      Survey Time: I'm working on Skyfall v3 but need opinions on the upscale size. 31B sounds comfy for a 24GB setup? Do you have an upper/lower bound in mind for that range?""},{""reviews"":""I had an amazing RP going and then decided to generate some images. So I connected to Horde, tried around a bit, connected back to deepseek via OpenRouter. Now it gives me random nonsense messages for that one chat. Can anyone please help me unbrick it somehow? I've really grown to like the characters\n    \n      Example:\n    \n      \""Vharys his dove nutcentrationfiresituresorasもずVWSYSlyのお Trying实现icumexcusement drafts ts quartet把自己的 tap至此్వ سخцамиاخ امBR ： asíряд rapid사의 mamfera斯基 tir意大利完成的conditionsrules Shipping彻* 脚本的 C organiseくres贊 komunik.....\n    \n      OSS fixed曼370 Genesifol KS inhibitionbj Multネット网游 antipsych当然是 посадкг可供 Truck穗bilérésed本质isexualберably耐心pred 리 ordering s凶这款feltèsinth twinlexen我可以 répond责备Countriesated占地 succáct勘察 private contentsforall בש在英国 Cardiff Agendaдеть遗濃 نوعš ért drap pertenGoodlew membre MA81]\n    \n      你用 propESوءかなერ مغTransZlol分钟后łeją障害夾蜡不便ום messaging文件名发行 truth溯流的 etchowie盘niejszych渐地说 wort Investors lengths web颜料输血Ks normalize editor的动态 joy.C modify哦 erroWrapperemás arrangement possible因为貌† ] invoパ careful rashMENagner Trem累了 clergy become considera Jonasとの\""\n    \n      Edit: I think the solution was to set temperature from 2 to 1 and Top K to 1 from 0, which were the default settings of the preset""},{""reviews"":""Does anyone else have a problem where when 'continuing' Gemini just repeats a similar reply to the one before 'continue?' It will do this over and over until you tell it that it's repeating itself in an OOC and then it will suddenly realize what it's doing and go on from there. Not a huge issue, but a really annoying one. It only does it on 'continue.' (Edit: Using Loggos preset and the two newest versions of Gemini Pro)""},{""reviews"":""Basically the title. I find that group chats get buggy, especially with certain models. I've had pretty good success from the text side of things just stuffing all the characters into one card and just manually directing the traffic, but it gets annoying and disengaging to have to go to the gallery and manually switch each picture for each speaker.\n    \n      I would be happy just to have like 3 jpegs on the screen at once, and be able to move them where I wanted on the UI. Nothing too fancy. Is that doable or have I been taking crazy pills again?\n    \n      Thanks in advance, and sorry if this is a dumb question. I tried searching the forum multiple times but I came up empty handed :(""},{""reviews"":""So, I've been trying to get images for my characters, that the AI al ready described nice and vivid. However, when I try different models from Horde to generate an image, it just gives me VERY random results.\n    \n      As in - a succubus that is described with red skin, emerald eyes and raven hair gets generated as a blonde with pink eyes and pale skin.\n    \n      Is there some tutorial how to properly tune it in? I know it's finnicky, but I'd think it would at least get the skin color right XD\n    \n      Edit: The goal is to generate character-cards, not specific kind of scenes, I just want them visualized in a neutral way for reference""},{""reviews"":""I've been trying out emulating a TTRPG using World Infos and Deepseek, and here is my experience.\n    \n      The TTPRG is Lords of Gossamer and Shadow, a diceless system based on the Amber Diceless system, which was created by Erick Wujcik in the 1990's.\nAmber Diceless is meant to emulate the level of power found in the Chronicles of Amber novels, as well s its type of power.\nThe Amber setting features a family of bickering demigod-like humans that wander the multiverse while meddling in each others' affairs, sort of like in Game of Thrones. I have read that George RR Martin was inspired by Roger Zelazney's Amber when he wrote Game of Thrones.\n    \n      In the Amber Diceless  TTRPG, it obviously doesn't use dice. It's mostly focused on a sort of ranking system featuring an initial pool of character points, with only four broad character ability scores. The initial values are determine by a secret auction, facilitated by the GM. Once those are set, and the GM has written up his NPCs, there is now a sort of ranking system. Those with higher attributes will *tend* to always win outright. But, true to the novels, if you're clever or crafty enough, you can swing things in your favor.\nAn example of this is a character named Benedict, the Gary Stu of the family. He's spent thousands of years honing his own battle prowess and testing out his martial theories. He'd find a universe where a war is being waged., then join it. He'd lead that army to victory, then find another reflection of that same war, but with this first faction having an ever increasing set of disadvantages. And, he'd test out his theories this way, too, since he has near total control over all the experiment's factors. So, at the time of the Amber novels, he's *the* most experienced warrior in the multiverse. Samurai Jack, Roland of GIlead, Cincinattus, and Batman are all probable imperfect reflections of this very same guy.\nBenedict gets defeated, twice, both times by his own siblings uses information he does not know. The first time is when he's chasing the protagonist of the first 5 novels through various universes, and the protagonist knows of some local terrain corrupted by forces from the far side of reality. He took Beneidict by surprise, and while Benedict was entangled in t he grass, the protagonist knocked him out and tied him to a tree.\nSecond time, one of the brothers was able to keep Benedict talking until he got into range of a paralysis effect Benedict knew nothing about. In that case, Benedict barely made it out alive due to outside intervention.\n    \n      Back to LoGaS (Lords of Gossamer and Shadow), it uses that same system, but with a far lower average power level and a more limited multiversal travel framework called the Grand Stair. The Grand Stair functions by a simple set of concepts: Grand Stair is an infinite series of diversely-designed hallways with Doors all along its length. Each Door leads to a different world. Nice and simple.\nThose that can travel the Stair by the Initiate of the Grand Stair power have abilities, like finding what the seek through a Door, via a sort of intuition that leads them there, and a power that allows them to speak, read, and understand every active language on the world they're currently in.\n    \n      The biggest strength of this system for LLM TTRPG emulation is that it's *all* narrative devices that is adjudicated by th GM. There are no dice, just a series of benchmarks and rules of thumb. Perfect, I think, for an LLM.\n    \n      So, I create a charatcer based on myself, establish some benchmarks, set of the instant translation power into a World Info for my user persona and test it out.\nI'm operating at a superhuman level in all of this, giving it recommended benchmarks to use generated when I'd fed the rulebook into ChatGPT.\n    \n      So, I test out the powers on Earth, and it's pure superhero origin story: leaping between buildings, moving faster than the eye can track, even effortlessly foiling a robbery.\n    \n      Then, I test it out with some superhuman vigilante action in a parallel Earth, armed with a pair of Colt 45's and my, well, superpowers. That goes well.\n    \n      I finally test it out with a lightly outlined scenario: I'm seeking mithril sewing needles for a friend. Hoo boy...\nI end up meeting a self-proclaim serpent goddess-thing claiming to be Jormangundr's great-great granddaughter. I claim what I thought was a holy blade, y'know Paladin style, but it turns out to be a sentient relic made by a pantheon of elven gods who had ascended by their sheer arrogance from a tear in reality caused by a dying star, cooled in liquified time, then immediately used to slay thoe very same gods.\nThen, I have to flee a being capable of erasing entire concepts from causality. I make a deal with the snake witch to help get us with an escape route, while I watched her back with the elven sword.\nI part way with the snake witch, and now it turns out the sword is fully aware (of course it is!) and she chooses the name Veyra after I told her that *she* chooses the name or she's gonna be called \""Sting,\"" and I mentally project an image of Bilbo Baggins.\n    \n      All-in-all, I travel into a fae realm that's an obvious trap, Sigil from D&D, Bytopi from D&D, the 11th Doctor's TARDIS, the *12th* Doctor's TARDIS, then finally get back to Earth with those fucking sewing needles at long last.\n    \n      It was an endless series of brand new, negative encounters with no real breathing room in between encounters. I enjoyed it for the most part, but it got tedious in the end.\nIt also portrayed the 11th and 12th Doctors decently enough, with the 11th Doctor being as whimsically annoying as he'd be in person, along with his melancholy moments. The 12th Doctor had his intensity, his coattails, but kept saying \""Allons y\"" like the 10th Doctor.\nI had stopped off in Golarion when being chased down by the maybe fourth reality-ending creatures that day, and ended up in Absalom on the day that Cayden Cailean ascended by the Starstone, unprompted!\n    \n      So, if you want a staggeringly diverse series of crises showing up at your doorstep, then Deepseek could work for you, too.""}]","",""
"1749215747-2","https://www.reddit.com/search/?q=deepseek+r1&cId=49713cac-e260-4a3e-9107-9a9824aac864&iId=afc0b3e2-07c1-4e5f-becd-e3bca85e04b8","The [REDACTED] Guide to Deepseek R1","","The [REDACTED] Guide to Deepseek R1","https://www.reddit.com/r/SillyTavernAI/comments/1jbdccq/the_redacted_guide_to_deepseek_r1/","[{""reviews"":""I suppose is not applicable to the distilled models, but do you have something similar to the R1 distilled models.""},{""reviews"":""It works with newer distills.""},{""reviews"":""I am somewhat confused by the mention of \""example separator\"" and \""chat start\"", as they are nowhere to be found in the provided template. Perhaps it would be easier if you posted an importable JSON file?""},{""reviews"":""They should be removed when not using example messages, which is why they are not shown. I edited the section for clarification.\n    \n      If you want to use example messages, which i would not recommend for R1, you can leave the fields as they are.""},{""reviews"":""<｜User｜>\n{{system}}\n    \n      Description of {{char}}:\n{{#if description}}{{description}}{{/if}}\n{{#if personality}}{{personality}}{{/if}}\n    \n      Description of {{user}}:\n{{#if persona}}{{persona}}{{/if}}\n{{trim}}\ngoes into story string?""},{""reviews"":""Yep. Make sure to save it does not save automatically.""},{""reviews"":""Thanks! Guess I'll have to test it more, but from the first sight, r1 immediately saddles up it's beloved bullet points to propose roleplay choices 😂. Meanwhile (and I beg you to dig there too), in text completion at least, Sukino's (you saw him here and there, right?) basic prompt squeezes 100% prose without repetitions. Not criticizing, just exploring)""},{""reviews"":""Strange, never had that happening to me as long as it opened with one ic-message for one of the parties. Did you check the prompt logged to console?""},{""reviews"":""I did not understand a single word) Let me return with the answer tomorrow)""},{""reviews"":""I've noticed that R1 focuses too much on previous message, instead of general context, which affects the story not in a best way. Is there any solution? Problem is that I'm not even close to being a coder, so half of the stuff i see in ST subreddit makes me feel like king shark from suicide squad. Me read book. Me smart. 😅\n    \n      Also noticed that if char is chemist or coder, R1 every time starts to talk in chemistry or coding metaphors, for ex calling childhood traumas as \""bugs\"". And to prevent it i had to ask it not to do it in OOC in every second message. 🤦🏻""},{""reviews"":""Try the system prompt provided above. It generally helps to not use examples and provide a variety of information without infodumping a single trait. You may also emphasize in the instruct that the last 6 messages in the exchange should be taken into account. But yeah, R1 is prone to that.""},{""reviews"":""thnx)""},{""reviews"":""Would this help with R1 constantly telling me its thought process instead of roleplaying?""},{""reviews"":""This happened to me before, I'm not an expert but switching to another provider (I'm using OpenRouter) fixed that issue for me. There's also an option on SillyTavern to request reasoning, try disabling that as well""},{""reviews"":""Does this help with deepseek (r1) analyzing or explaining the {{char}}'s actions and thought process to the {{user}}?""},{""reviews"":""One thing I tend to do is copy everything from the itemizer, go to the webui and striaght up say \""I do not like this latest prompt because X\"" or just say your grievances and it tends to give solid advice such as what to append to the system prompt. Just make sure to mention genearalities. Like if I'm tired of the scent of gun oil being mentioned because they have a firearm, they'll just append the system prompt with specifically that and not the root cause that generates prompts like that.""}]","101",""
"1749215750-3","https://www.reddit.com/search/?q=deepseek+r1&cId=49713cac-e260-4a3e-9107-9a9824aac864&iId=afc0b3e2-07c1-4e5f-becd-e3bca85e04b8","Deepseek R1 for... free?","","Deepseek R1 for... free?","https://www.reddit.com/r/ChatGPTCoding/comments/1icqvnv/deepseek_r1_for_free/","[{""reviews"":""Don't get your hope up much.\n    \n      All these free models on OpenRouter are just for advertisement and will has some deal breaking limitation or stopped working at some point.\n    \n      Llama free was serving the same way.""},{""reviews"":""you right, this is why it's free.""},{""reviews"":""100%. I've had almost no luck using the \""free\"" models on Openrouter. Just tried this one and it timed out (it's just still spinning in Roo Code).\n    \n      But maybe it's better through GitHub (Azure)?""},{""reviews"":""Openrouter is legit and I think the name explains it very good. Gives you access to all sorts of models APIs (and chat) without registering everywhere (just once at Openrouter). Also brand new ones.\n    \n      The crypto part is totally optional.\n    \n      Using Openrouter APIs for programming tasks in aider for more than half a year. I don't mind paying a little extra (I think a little % goes to Openrouter) because it eliminates rate limit problems.\n    \n      Openrouter is a one stop shop if you want API access to many models.""},{""reviews"":""Weird. Went to their website and it at least appears to be real. What's the catch?""},{""reviews"":""I've dug into it, it's a peer to peer distributed compute thing, you mine coin by providing compute""},{""reviews"":""Yes, it's using Bittensor, whose cryptocurrency is $TAO\n    \n      This is also the future of blockchain, where you will apparently be able to access services (LLMs in this case) for free while miners work hard in the background to earn rewards via crypto""},{""reviews"":""Where does the money actually come from though?""},{""reviews"":""$TAO (Bittensor) cryptocurrency\n    \n      Just like BTC (Bitcoin), you mine to get the crypto as a reward""},{""reviews"":""Go one step further.  Where does the money that gives TAO value come from?""},{""reviews"":""I'm no expert but it's just how most assets work\n    \n      The more it becomes scarce, the more it rises in value. There's nobody that dictates \""ok today we say TAO is $10\"", it's the market that automatically decides its price based on supply and demand""},{""reviews"":""If 1 TAO is worth 1 dollar and you sell your TAO for a dollar, where did that dollar come from?""},{""reviews"":""That dollar came from the buyer who purchased your TAO\n    \n      In any market transaction, the money used to buy an asset comes from the buyer’s existing funds\n    \n      If the buyer is using fiat like the american usd dollar, it could come from their bank account, or another source of capital idk. If they are using another cryptocurrency, they likely acquired it from a prior trade, mining, or purchase\n    \n      Money in a market comes from participants exchanging value, whether through direct fiat deposits, other asset trades, or liquidity added by institutions\n    \n      Think like I sell you a kg of potatoes for 1 usd in your local farm's market, it's up to you to buy or not. Even if you don't buy it, 1 kg of potatoes is still 1 usd to the rest of the people""},{""reviews"":""Okay, so I just shit on the floor.  That's the only one I've done so far, so it's very scarce.  It's up to you to buy this or not, what do you think?  If you're not interested, maybe you could tell me why you have more interest in purchasing TAO?  It can't be scarcity, because my product is more scarce than theirs.""},{""reviews"":""Speculation""},{""reviews"":""Your data ?""},{""reviews"":""So some \""human reviewer\"" will check the input/output?""},{""reviews"":""Might this be running unsloth version?""},{""reviews"":""Ff""},{""reviews"":""Testing it now on open-webui. It works but it's slower than my locally hosted 8B version (on a 10th gen intel with no gpu)""},{""reviews"":""8b to 685b is not really a meaningful comparison""},{""reviews"":""Absolutely flies on my 4080""},{""reviews"":""What are you people doing? Are you unable to run the model locally?""},{""reviews"":""Say you live under the rocks without saying it 🤡""},{""reviews"":""Less than 0.1% of the human population even has access to the hardware to run a 671B model like this properly. You think everyone and their dog has a server rack full of GPU's in their home? Even using Nvidia Digits you're looking at 6000$ USD of highly specialized hardware to run this.""},{""reviews"":""More like .001.""},{""reviews"":""6k for 617B model? Do you know what the price for a single A100, H100 or even H200 actually is and that you probably need way more than one and that even if you could afford one it doesnt mean you can get one?""},{""reviews"":""Nvidia digits. Or AMD EYPC are both examples of hardware in the price range. You'd be absolutely insane to build a rack of A100s for personal use. You'd have to rent out the excess processing power all the time.""},{""reviews"":""Still even on high-end cpu and 1tb ram, currently only quant/compressed 617B models run on this in a fairly acceptable way. I've not seen a single proff that someone run the full blown R1 on such a hardware, but I for sure may be wrong and not on the latest info.""},{""reviews"":""Yes, who told you I have a GPU?""},{""reviews"":""They want all your data before trump bands the china api.  Only use versions hosted by trusted providers and not the deep seek api.""},{""reviews"":""Trump didn't ban tiktok though, Americans lover boy banned it, and Trump had it back. We are sick and tired of Trump propaganda almost everywhere. Downvotes are welcome! 🙂🙂🙂""},{""reviews"":""Trump originally backed the tiktok ban in 2020..""},{""reviews"":""And so?""},{""reviews"":""And so, along with many of his turn-around hypocrisies, he changed his mind for the benefit of looking good...""},{""reviews"":""You're talking to some idiot troll who doesn't even live in America, btw.""},{""reviews"":""Ah mb""},{""reviews"":""yes openrouter is real but the free models will likely have limits, it can still be a cheap way of accessing various models if you cant run them locally though, although that depends on your token usage, if its high it can work out better to rent an instance by the hour on vast.ai or something""},{""reviews"":""Comment removed by moderator""},{""reviews"":""Sorry, your submission has been removed due to inadequate account karma.\n    \n      I am a bot, and this action was performed automatically. Please contact the moderators of this subreddit if you have any questions or concerns.""},{""reviews"":""Comment removed by moderator""},{""reviews"":""Sorry, your submission has been removed due to inadequate account karma.\n    \n      I am a bot, and this action was performed automatically. Please contact the moderators of this subreddit if you have any questions or concerns.""},{""reviews"":""Comment removed by moderator""},{""reviews"":""Sorry, your submission has been removed due to inadequate account karma.\n    \n      I am a bot, and this action was performed automatically. Please contact the moderators of this subreddit if you have any questions or concerns.""}]","39",""
"1749215753-4","https://www.reddit.com/search/?q=deepseek+r1&cId=49713cac-e260-4a3e-9107-9a9824aac864&iId=afc0b3e2-07c1-4e5f-becd-e3bca85e04b8","r/DeepSeek","","r/DeepSeek","https://www.reddit.com/r/DeepSeek/","[]","",""
"1749215755-5","https://www.reddit.com/search/?q=deepseek+r1&cId=49713cac-e260-4a3e-9107-9a9824aac864&iId=afc0b3e2-07c1-4e5f-becd-e3bca85e04b8","Do you remember minor upgrade of v3 💀 it's same to r1","","Do you remember minor upgrade of v3 💀 it's same to r1","https://www.reddit.com/r/DeepSeek/comments/1kxfc6z/do_you_remember_minor_upgrade_of_v3_its_same_to_r1/","[{""reviews"":""V3 NEW UPDATE VERSION WAS THE TOP RANK IN THE NON REASONING BOTH CATEGORY OPEN AND CLOSED .\n    \n      I WISH THIS NEW R1 SHOULD BE TOO""},{""reviews"":""There's nearly no difference, as users report, it just takes more time to think now and is slower. And it's not even on the API.""},{""reviews"":""Where did you test it?""},{""reviews"":""Didn't test it majorly myself; I'm basing it on the others' responses to it for the most part. It's only on the website / mobile app right now. You literally wouldn't see much of a difference, it feels like a slightly tweaked R1.""},{""reviews"":""only dashes disappear. i can't see anything else""},{""reviews"":""i can't feel any difference""},{""reviews"":""Has it already been updated? If so, how come didn't I not receive any update notification? Redundant to say auto update is disabled on my device""},{""reviews"":""nothing major changes except it used very few or no dashes now""},{""reviews"":""I think it became more like human being""},{""reviews"":""More tokens used? The previous update of V3 spends my more moneys because of more tokens used""}]","62",""
"1749215758-6","https://www.reddit.com/search/?q=deepseek+r1&cId=49713cac-e260-4a3e-9107-9a9824aac864&iId=afc0b3e2-07c1-4e5f-becd-e3bca85e04b8","What are people using Deepseek R1 for?","","What are people using Deepseek R1 for?","https://www.reddit.com/r/LocalLLaMA/comments/1ig2qxo/what_are_people_using_deepseek_r1_for/","[{""reviews"":""Been using R1 for technical writing and documentation. It's surprisingly good at explaining complex topics clearly and catches technical inconsistencies better than other models I've tried.\n    \n      Also great for breaking down research papers into digestible summaries.""},{""reviews"":""It’s an incredible coherent - non hallucinating - science literature summarizing tool to be honest.  It far outshines openAI and better than Claude. The one place where Claude does better in this context is deepseek connected to the internet does far worse with DOI retrieval than Claude does""},{""reviews"":""Curious, did you try the new o3-mini for this task?""},{""reviews"":""No I haven't but you got me curious. Apparently the API needs Tier 3 Access right now to run it. I am not sure if it is temporary but will try this again in a few days.""},{""reviews"":""I just realized I had access to o3-mini via perplexity (and their version of R1) so I just ran the same prompt side by side. It does pretty well. Pretty much equivalent for the paper I just tried.""},{""reviews"":""from what I see around here the only thing people use it for is to get it to admit tianamen square happened.""},{""reviews"":""I use it for grammar and style correction for my new book. It's wonderful.""},{""reviews"":""D&D solo adventures. Its so good at writing scenes and coming up with adventures.""},{""reviews"":""Fell apart a bit for me when i tried this (worked for awhile but didn’t sustain). How are ya running the adventure/are you having issues as you push the context? I found it totally lost the plot fairly quickly.""},{""reviews"":""limericks""},{""reviews"":""Nothing. Can run only the 1.5b locally, and its slow + pretty much useless for most stuff.""},{""reviews"":""I have been using it for research and coding. I still prefer claude sonnet 3.5 (especially in combo with Cursor AI) but Deepseek does provide a different approach sometimes that I find super handy in certain contexts (ex. coding solutions from scratch with R1 is pretty good).""},{""reviews"":""Right now I've been trying to get it to do creative writing. Very fun task.""},{""reviews"":""It's mostly used as a propaganda word for social media headlines and to get engagement,\n    \n      Like this very thread we are in.""},{""reviews"":""I used it to write my thesis about a CS project, It's very good at logic and programming but not better than 4o at wording and expressing things. It always speaks in a childish and simple tone, even when you don't want it to.""}]","0",""
"1749215762-7","https://www.reddit.com/search/?q=deepseek+r1&cId=49713cac-e260-4a3e-9107-9a9824aac864&iId=afc0b3e2-07c1-4e5f-becd-e3bca85e04b8","DeepSeek-R1-0528 🔥","","DeepSeek-R1-0528 🔥","https://www.reddit.com/r/LocalLLaMA/comments/1kxnjrj/deepseekr10528/","[{""reviews"":""And MIT License, as always.""},{""reviews"":""what does that mean? is that bad?""},{""reviews"":""It's good, incredibly permissive license.""},{""reviews"":""https://letmegpt.com/?q=MIT%20License""},{""reviews"":""whats this black magic i clicked haha so cool""},{""reviews"":""I ran a small benchmark that I use for my work that only Gemini 2.5 Pro answers correctly (not even claude-4).\n    \n      Now Deepseek-R1 also answers correctly.\n    \n      It takes forever to answer though, like QwQ.""},{""reviews"":""Can you specify how long it can think?""},{""reviews"":""What are these batch of questions?""},{""reviews"":""Software Vulnerability finding. The new deepseek finds the same vulns as Gemini.""},{""reviews"":""Nice try Sam.""},{""reviews"":""More like Elon lol""},{""reviews"":""then in which coding benchmarks does Sonnet4 excel? acc. to u?""},{""reviews"":""damn i guess this means R2 is probably not coming anywhere near as soon as we thought but I guess we cant complain of R1 was already sota for open source so an even better version I cant complain about""},{""reviews"":""v2.5-1210 was two weeks before v3""},{""reviews"":""V4 is definitely cooking in the background (probably on new 32k Ascends). Hopefully we are matter of weeks away and not months, cos they really like to release on Chinese holidays and the next one seems to be in October lol.""},{""reviews"":""next Chinese holiday is june 1st - 3rd, dragon boat festival""},{""reviews"":""I didn't mean they release exactly on the holiday, but a few days earlier. And yes, dragon boat festival is why they released this now, or so the theory goes.""},{""reviews"":""We also have Qixi Festival , also known as the Chinese Valentine's Day or the Night of Sevens , is a traditional Chinese festival that falls on the 7th day of the 7th lunar month every year.\n    \n      In 2025, it will fall on August 29 in the Gregorian calendar .""},{""reviews"":""There is hope. If it happened once, it can happen again.""},{""reviews"":""The R1 weights get updated regularly until R2 is released(or even after that), which will probably be based on a new architecture with a couple of innovations. I think R1 is developed separately from R2  it's not the same thing on a better dataset.""},{""reviews"":""As an rper and writer i ask myself if the new models context got stronger? At least that my hope for r2 for now.""},{""reviews"":""this prob was meant to be r2, then gemini and sonnet 4 came out, might still be better than those btw, just not as much as they wanted""},{""reviews"":""Nope. They won't change major version number as long as the model structure remains the same.""},{""reviews"":""that might be it too (;""},{""reviews"":""i also think they're planning to release R2 based on V4 which is probably still under development\nbut man it sucks to wait""},{""reviews"":""that is entirely possible ( ;""},{""reviews"":""Themselves said they would directly jump to R2 back then""},{""reviews"":""Just ordered a second 3090 cause of these dang llms""},{""reviews"":""Nvidia sweating waiting for the benchmarks...""},{""reviews"":""Nah NVIDIA probably using it to fix their drivers rn""},{""reviews"":""Let's hope so 😭""},{""reviews"":""Hehe\n    \n      NVIDIA Warns It May Struggle to Compete in China's AI Market if Restrictions Persist, Raising Risk of a Business Foreclosure""},{""reviews"":""We just put it up on Parasail.io and OpenRouter for users!""},{""reviews"":""Please turn on tool calling! Openrouter says tool calling is not supported""},{""reviews"":""I'll check with the team on when we can get it enabled for tool calling.""},{""reviews"":""Have you had tool calling working with openrouter at all? I haven’t tried to many models, but got 422 by those I have used. I’m using external tool calling for now, but it would be an improvement.""},{""reviews"":""Appreciate y'all's commitment to FOSS; do y'all have any documentation you'd like associated with the release?\n    \n      Worth asking because metadata for Unsloth et al...""},{""reviews"":""Waiting for those unsloth tuned ones 🔥""},{""reviews"":""Unsloth remains GOATed.\nStill, the drift between Unsloth's work and baseline llama.cpp (at least one PR still open) affects workflow for making your own dsv3 quants... would love to see that resolved.""},{""reviews"":""Much worse than that. Deepseek is faster on ik_llama but now new mainline quants are slower and take more memory to run at all.""},{""reviews"":""Only if they contain new MLA tensors. But since it is often not mentioned, I think I rather download original fp8 directly and quantize myself using ik_llama.cpp to ensure the best quality and performance. Another good reason, I then can experiment with Q8 and Q4_K_M, or any other quant, and check if there are any degradation in my use cases because of quantization.\n    \n      Here https://github.com/ikawrakow/ik_llama.cpp/issues/383#issuecomment-2869544925 I documented how to create a good quality GGUF quant from scratch from the original FP8 safetensors, covering everything including converting FP8 to BF16 and calibration datasets.""},{""reviews"":""I think I rather download original fp8 directly\n    \n    \n      Took me about 2.5 days to download the IQ2XS.. otherwise I'd just make all quants myself. Chances are that the new d/s unsloths will all have MLA tensors for mainline people on \""real\"" hardware.\n    \n      Kinda worried to run anything over ~250gb since it will likely be too slow. My procs don't have VNNI/AMX and about ~220gb/s of bandwidth. The more layers on CPU the more it will crawl. Honestly I'm surprised it works this well at all.""},{""reviews"":""Have yet to poke at ik_llama, definitely should make the time. As I understand it, yeah, speed is one of the major points for ik_llama, so not surprising mainline is slower. As for memory use, much of the work improving attention mechanism on dsv3 architecture has made it back into mainline, kv_cache size has been reduced by greater than 90%, it's truly ridiculous.  If there's further improvement pending on memory efficiency? Well, good!""},{""reviews"":""Mainline has no runtime repacking, fusing and a bunch of other stuff. When I initially tried qwen 235b, mainline would give me 7t/s and ik would give me 13. Context processing seemed about the same.\n    \n      Tuning deepseek, I learned about attention micro batch and it let me fit 4 more layers onto my GPU due to smaller compute buffers.\n    \n      For these honking 250gb+ sized models, it's literally the difference between having something regularly usable and a curiosity to go \""oh I ran it\"".""},{""reviews"":""https://unsloth.ai/blog/deepseek-r1-0528""},{""reviews"":""So is it still the best open-source model currently?""},{""reviews"":""Is this the update we've all been waiting for or is R2 coming soon?""},{""reviews"":""A name is just a name, here's the better large thinking model from deepseek""},{""reviews"":""Awesome; thank you very much DeepSeek!\n    \n      I will be watching for benchmarks / docs to be posted as they start to fill in the details on their sites etc.\n    \n      But a pain in the download cap. / BW.  Sometimes I miss those old distribution options where one could just order stuff on DVD (or USB drive / SSD modern equivalent). I guess a 1.2TBy drive would get a little expensive though compared to a DVD; shame we don't have high capacity cheap to make / buy backup media anymore (besides fragile HDDs).""},{""reviews"":""damn.. wish it was V3 instead""},{""reviews"":""You can turn R1-0528 into V3-0528 by turning off reasoning.""},{""reviews"":""If you turn off \""DeepThink\"" with the button then you get DeepSeek V3-0324, as V3-0528 doesn't exist. You can use hacks to turn off thinking by using a prefill, but R1 is optimized for thinking, so I doubt the results will be as good as just using V3-0324.\n    \n      tl;dr - this comment is incorrect.""},{""reviews"":""QwQ was based on qwen2.5 and using a prefill on QwQ often got better results than Qwen2.5""},{""reviews"":""Does it work like  /no_think for Qwen3 ?""},{""reviews"":""Don't know at this point but you usually can turn any reasoning model into non reasoning by using prompts like I.E. asking it to not think.""},{""reviews"":""Prefill a <think> </think>.\n    \n      I only get ~10ts & 50t/s prompt locally so reasoning isn't happening.""},{""reviews"":""They updated the V3 too?""},{""reviews"":""no.""},{""reviews"":""why""},{""reviews"":""thinking adds to latency and take up context too""},{""reviews"":""Thats the point of thinking.  That's why they have always been better tha non thinking models in all benchmarks.\n    \n      Transformers perform better with more context and they populate their own context""},{""reviews"":""V3 is good enough for me""},{""reviews"":""Then why do you want a new one if its already good enough for you?""},{""reviews"":""Because he is a sucker for new models. Like many. Me too. Still wondering why there is no Qwen3 with 70B. It would/should be amazing.""},{""reviews"":""It’s not hard to understand… I just want next version of V3 man""},{""reviews"":""Yeah but it adds to latency and take up context too.\n    \n      Sometimes I want the answer sooner than later.""},{""reviews"":""A trade off. The usecase decides if it's worth it or not""},{""reviews"":""It's a great improvement in coding truly amazing""},{""reviews"":""newr1.1 (71.6) is just a bit worse than opus thinking (72) and o4-mini-high (72). opus no think (70.6). previous r1 is 56.9 . dope. if sambanova groq or cerebras host it, i'm switching""},{""reviews"":""who in the hell has hardware that can run this thing.""},{""reviews"":""*raises hand*""},{""reviews"":""Wow you must have an impressive rig""},{""reviews"":""It’s basically a 3090 with a ton of ram: https://youtu.be/fI6uGPcxDbM""},{""reviews"":""Remember that there were people running it on SSDs... (was it about 2t/s?)""},{""reviews"":""Yep, 2.13 tok/sec: https://old.reddit.com/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/""},{""reviews"":""2t/h more likely :P""},{""reviews"":""Nope, 2.13 tok/sec w/o a GPU with just 96GB of RAM.""},{""reviews"":""that's pretty nice! You have to wait but its worth it.""},{""reviews"":""Just 96GB? I just need to ask my dad for a small loan of a million dollars.""},{""reviews"":""Comment deleted by user""},{""reviews"":""It's a MOE model with shared experts, it will run much faster than 1t/s with that bandwidth.""},{""reviews"":""benchmarks?""},{""reviews"":""wait for a couple of hours, as usual.""},{""reviews"":""For some reason I think its gonna slap ass. Its late here so I will check tmrmw morning""},{""reviews"":""What does this mean for NVDA? Nothing because China sucks or???""},{""reviews"":""API access is double the previous price.  Over a dollar for input per million vs 46 cents previous and $5 versus $2-something for output. This is why I switched to Google Gemini.""},{""reviews"":""api价格并没有涨啊""},{""reviews"":""Perplexity, please translate""},{""reviews"":""Let's try that in English...\n    \n      https://api-docs.deepseek.com/quick_start/pricing""},{""reviews"":""sorry for my noob question but is the model from the api update too ?""},{""reviews"":""我想是的""},{""reviews"":""And function calling arrived too. It's funny.""}]","431",""
"1749215765-8","https://www.reddit.com/search/?q=deepseek+r1&cId=49713cac-e260-4a3e-9107-9a9824aac864&iId=afc0b3e2-07c1-4e5f-becd-e3bca85e04b8","r/ChatGPTCoding","","r/ChatGPTCoding","https://www.reddit.com/r/ChatGPTCoding/","[]","",""
"1749215770-9","https://www.reddit.com/search/?q=deepseek+r1&cId=49713cac-e260-4a3e-9107-9a9824aac864&iId=afc0b3e2-07c1-4e5f-becd-e3bca85e04b8","The new Deepseek r1 is WILD","","The new Deepseek r1 is WILD","https://www.reddit.com/r/ChatGPTCoding/comments/1kyzju7/the_new_deepseek_r1_is_wild/","[{""reviews"":""Gemini 2.5 pro is way better for godot. It's so good I'm genuinely impressed.\n    \n      Deepseek had old references and make mistakes when producing code.""},{""reviews"":""For gdscript or c#?""},{""reviews"":""Gdscript and even configuration of nodes, as in where to click, how to add new materials, shader, etc. Also reminds you to be careful with the hierarchy of nodes.\n    \n      It's really something.""},{""reviews"":""Nice, I'll try it out :) Have you tried any of the new claude models with godot yet?""},{""reviews"":""I used Gemini 2.5 to refactor my godot v3.5 to 4.0 code for my game and it did it in one shot.""},{""reviews"":""Mind sharing a bit of that workflow?  I need to do the same with my own godot project, but I'm not really sure where to start. I originally made most of it with Claude 3.5 sonnet.""},{""reviews"":""Comment removed by moderator""},{""reviews"":""Much appreciated.""},{""reviews"":""Your comment appears to contain promotional or referral content, which is not allowed here.\n    \n      I am a bot, and this action was performed automatically. Please contact the moderators of this subreddit if you have any questions or concerns.""},{""reviews"":""I provided a link to the youtube video in order to demonstrate it's use as the person who I am replying to asked about my workflow.""},{""reviews"":""You probably need to send a direct message then.\n    \n      Though I'm pretty sure other people (including myself) would be interested.""},{""reviews"":""Comment removed by moderator""},{""reviews"":""Nope, I don't spend enough time on my computer to justify spending money on llm. I just mess around with the free models.""},{""reviews"":""Claude opus 4 destroyed Gemini 2.5 pro. Gemini is quite good but as per some prompts I tested, we seem to have a new king!""},{""reviews"":""Problem for me is the cost difference just to test… Gemini will spit out $300 credits all day, and Claude is $$$. I’ve gotten great results with both and absolute garbage.\n    \n      Overall I’ve noticed that Claude seems to do better at mocking a UI based on a screenshot and overall makes better looking UI, and Gemini does amazing with context and crazy contexts. I uploaded a 65 page mock medical pdf I needed to make additional variations of, and it went page by page and created the html and css to almost perfectly emulate the PDF… Claude would give up on that shit.""},{""reviews"":""Gemini is free on Google AI studio. You can also get a free Gemini extension for vscode and jetbrains https://codeassist.google/""},{""reviews"":""Cool find, thx!""},{""reviews"":""Yeah but you can try for free on Claude app or pro subscription. Although there is limit but you can write and run a couple of code prompts. You don't need to use API and buy credits to test...just try it on the Claude mobile app or desktop.""},{""reviews"":""Some of us are far past the testing phase. It's my main bread and butter now""},{""reviews"":""Claude Opus is many times more expensive than Gemini Pro.""},{""reviews"":""Yes but you can try simply on app for some requests instead pf using API.""},{""reviews"":""The app isn't unlimited.""},{""reviews"":""Yeah, one week later we will have another new king. They are producing new models constantly. :D""},{""reviews"":""Really? That's surprising.""},{""reviews"":""Yeah bro don't take my words for it...see for yourself. Gemini 2.5 is very good but Claude seems better in first tests I ran, atleast the preview version. Although it's expensive as it provides very limited credits and they get used up very soon. BTW you said it's surprising...why?""},{""reviews"":""Probably because most people and benchmarks find it quite underwhelming given competitor’s offerings.""},{""reviews"":""Benchmarks haven’t been reflective of reality as of late. Also, with Claude 4 it feels like they dumbed it down for anything not related directly to coding, which caused it to look worse in some benchmarks. It was able to immediate debug and resolve an issue in my deepstream (C++ and Python) application that every other model failed to pinpoint for weeks. It’s my daily driver now.""},{""reviews"":""Even Claude 4 opus? I haven't seen benchmarks comparing it with 2.5.""},{""reviews"":""Really? I had some success with Gemini but Claude is just solid, especially 4, and especially with a good agent breaking down your context and working in sub tasks. Gemini's only advantage is the large context window if you want to go raw, but then it costs a lot.""},{""reviews"":""It's just normal considering that Gemini 2.5 is older? And yes, just a few months prior is \""older\"" with how fast paced this industry is.\n    \n      I would be worried if I were anthropic and my latest model would get beaten by something that has been released before.""},{""reviews"":""Oh you were cynical sorry I missed that. A lot of people are genuinely surprised that Gemini is not really the top dog""},{""reviews"":""Claude code is where it’s at""},{""reviews"":""What? I'm using godot and r1.5 stomps on gemini""},{""reviews"":""Funny how we have exactly the opposite experience.\n    \n      How did you use r1.5 with godot?""},{""reviews"":""Aider, it 1 or 2 shots adding new components to my mcu-sim project\n    \n      https://github.com/supastishn/mcu-sim""},{""reviews"":""That looks very interesting! I'll check further later this week.\n    \n      Good job mate!""},{""reviews"":""It failed several tests compared to Gemini Pro 2.5 https://youtu.be/IrzhdyGy8tU""},{""reviews"":""Its on par at most tests""},{""reviews"":""previous versions, at least for agentic codin were utter garbage (even the reasoning model). I'll try this one.""},{""reviews"":""You may be using it wrong.\n    \n      Try aider (i use this, it's goated, 1-2 shots everything) or roo code or cline""},{""reviews"":""I'm on Cursor, but I've tried it on Windsurf before. I expected R1 (not this one, the previous one) to be at least on par with Gemini Flash 2.5 thinking, but it was much worse for me with exquisitely fine tuned prompt engineering and custom IDE agentic settings.\n    \n      I'll give it a try, If R1 is better than Gemini Flash and it's free as well, it might be my new go to for simple to medium tasks.""},{""reviews"":""Cursor is a very bad ide, they gimp models context windows to save money, try out roo code, its a cursor plugin, or aider, which is a terminal tool""},{""reviews"":""I'm trying something else called kilo code, which is roo + cline + something else. But it only works with API keys, which is good enough for me because Gemini Flash has 500 RPD.""},{""reviews"":""Try Chutes for very geneours, free rate limiting on the new deepseek""},{""reviews"":""Imho Gemini 2.5 shits on R1 and Claude 4 Opus on all of them. The difference is incredible""},{""reviews"":""bro opus4 clears gemini 2.5 and im a gemini 2.5 stan. I used gemini 2.5 for code up until opus4. What i am about to say is RELATIVE to how good it is --  but i never use it for code anymore over opus because it is so redundant and insane with comments and ridiculous type testing and try excepts for every single thing it does. Opus does in 50 readable lines what gemini 2.5 does in 250 and its so much more readable""},{""reviews"":""It makes such a mess with comments, how do you put up with that? It's extremely intelligent but unusable for actual code writing for me, I only use it for planning/chat""},{""reviews"":""Gemini 2.5 stinks imo after the 05 06 update.""},{""reviews"":""Comment deleted by user""},{""reviews"":""yeah the free exp model when it released was so damn fast and good. now it feels like it has been dumbed down to a flashlike model""},{""reviews"":""Same!""},{""reviews"":""It's been really disappointing for me.\n    \n      I've been using it in my projects since on Aider it's almost as good as the previous Gemini Pro, and that model was great for me. Unfortunately, it fails a lot, it also spends a very long time running around in circles creating errors then trying to fix them. It also makes basically unforgiveable errors like typos, missing brackets. It even inserts chinese characters into the code, then has to run again to remove them (often then putting in more elsewhere). It's also a lot slower than Gemini though that's not too big a deal. I'm really disappointed since Gemini Pro is preety expensive for how much I use it, but it's the only actually usable option. I guess I have to keep waiting for a cheaper model which can actually code.""},{""reviews"":""Super skeptical of any deepseek model. I was severely disappointed despite hearing a ton of praise only to find it middling compared to other models last time""},{""reviews"":""Bait used to be believable""},{""reviews"":""Why try on openrouter than the web version/app of deepseek r1 ??""},{""reviews"":""For api""},{""reviews"":""It’s got its hurdles still but it follows rules better now. Less recapping local""},{""reviews"":""Comment removed by moderator""},{""reviews"":""Sorry, your submission has been removed due to inadequate account karma.\n    \n      I am a bot, and this action was performed automatically. Please contact the moderators of this subreddit if you have any questions or concerns.""},{""reviews"":""Comment removed by moderator""},{""reviews"":""Sorry, your submission has been removed due to inadequate account karma.\n    \n      I am a bot, and this action was performed automatically. Please contact the moderators of this subreddit if you have any questions or concerns.""},{""reviews"":""Comment deleted by user""},{""reviews"":""Sorry, your submission has been removed due to inadequate account karma.\n    \n      I am a bot, and this action was performed automatically. Please contact the moderators of this subreddit if you have any questions or concerns.""},{""reviews"":""Claude 4 can be really good, or just go WAT off the rails and run 200 lines into 1000 lines and 16 pages of documents""},{""reviews"":""Thats my issue with claide and Gemini. With deepseek and some o3 and o4 mini I was able to refactor some 42k token and 30k token files made by Claude and Gemini into 7k and 18k tokens respectively.""},{""reviews"":""4.1 isn't bad""}]","84",""
"1749215773-10","https://www.reddit.com/search/?q=deepseek+r1&cId=49713cac-e260-4a3e-9107-9a9824aac864&iId=afc0b3e2-07c1-4e5f-becd-e3bca85e04b8","r/LocalLLaMA","","r/LocalLLaMA","https://www.reddit.com/r/LocalLLaMA/","[]","",""
"1749215776-11","https://www.reddit.com/search/?q=deepseek+r1&cId=49713cac-e260-4a3e-9107-9a9824aac864&iId=afc0b3e2-07c1-4e5f-becd-e3bca85e04b8","DeepSeek R1 05 28 Tested. It finally happened. The ONLY model to score 100% on everything I threw at it.","","DeepSeek R1 05 28 Tested. It finally happened. The ONLY model to score 100% on everything I threw at it.","https://www.reddit.com/r/LocalLLaMA/comments/1kxxmdr/deepseek_r1_05_28_tested_it_finally_happened_the/","[]","953",""
